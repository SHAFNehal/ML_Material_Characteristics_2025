{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Set Code to access other codes\n",
    "\n",
    "# Path to code files\n",
    "sys.path.append(r'./main_codes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required library\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "from data_processing import pre_process_data, pre_process_data_no_split\n",
    "from regression_model import train_evaluate_models, regressors, param_grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_excel('./data/Formatted_Data_v2.xlsx').drop(columns=['Width1', \n",
    "                                                                    'Width2', \n",
    "                                                                    'Diameter',\n",
    "                                                                    'Height',\n",
    "                                                                    'CAD_Volume'])#.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEcessary Variables\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# Input Data\n",
    "X = data.drop(columns=[\n",
    "                         'Sample_Apparent_Density', \n",
    "                         'Sample_Relative_Density', \n",
    "                         'Sample_Shrinkage'\n",
    "                    ])\n",
    "\n",
    "# Output Data\n",
    "y_rel_den = data['Sample_Relative_Density']\n",
    "y_sam_shrink = data['Sample_Shrinkage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature List\n",
    "columns_to_use = ['Actual_Volume', \n",
    "                  'Sample_Size', \n",
    "                  'Sample_Saturation', \n",
    "                  'Sample_Delay',\n",
    "                  'Sample_Prints', \n",
    "                  'Sample_Geometry']\n",
    "\n",
    "categorical_features    = [\n",
    "                            'Sample_Size', \n",
    "                            'Sample_Prints', \n",
    "                            'Sample_Geometry'\n",
    "                           ]\n",
    "\n",
    "numerical_features      = [\n",
    "                            'Actual_Volume',\n",
    "                            'Sample_Saturation', \n",
    "                            'Sample_Delay',\n",
    "                            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered_features = numerical_features + categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Density [y_rel_den]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed, y_train, preprocessor_pipeline = pre_process_data_no_split( numerical_features = numerical_features,\n",
    "                                                                                 categorical_features = categorical_features,\n",
    "                                                                                 X = X,\n",
    "                                                                                 y = y_rel_den,\n",
    "                                                                                 columns_to_use = reordered_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_pipeline.transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the fitted OrdinalEncoder\n",
    "ordinal_encoder = preprocessor_pipeline.named_transformers_['categorical']\n",
    "\n",
    "# Get the categories for each feature\n",
    "category_mapping = ordinal_encoder.categories_\n",
    "\n",
    "# Display the mapping\n",
    "for feature, categories in zip(categorical_features, category_mapping):\n",
    "    print(f\"{feature}: {list(enumerate(categories))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cross_validate(X, y, numerical_features, categorical_features, reordered_features, filename, random_state = 420, folds = 5, shuffle = False):\n",
    "    # Transform the Train Test Data\n",
    "    X_train_transformed, y_train, preprocessor_pipeline = pre_process_data_no_split(numerical_features = numerical_features,\n",
    "                                                                                    categorical_features = categorical_features,\n",
    "                                                                                    X = X,\n",
    "                                                                                    y = y,\n",
    "                                                                                    columns_to_use = reordered_features)\n",
    "    \n",
    "    # THe plan is the check the performance on KFOLD Data\n",
    "    kfold = KFold(n_splits=folds, shuffle=shuffle, random_state = random_state)\n",
    "\n",
    "    # Initialize lists to store model results\n",
    "    model_results_rel_den = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for fold, (train_idx, val_idx) in tqdm(enumerate(kfold.split(X_train_transformed)), desc = 'Fold'):\n",
    "        # Split the data for this fold using list comprehension\n",
    "        # X_train_transformed, y_train <-- Pandas Data Frame\n",
    "        X_fold_train, X_fold_val = X_train_transformed.iloc[train_idx], X_train_transformed.iloc[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        # Train and evaluate all models\n",
    "        results_rel_den, _  = train_evaluate_models(regressors, \n",
    "                                                    param_grids,\n",
    "                                                    X_fold_train, \n",
    "                                                    y_fold_train, \n",
    "                                                    X_fold_val, \n",
    "                                                    y_fold_val)\n",
    "        # Append the results to the main list\n",
    "        model_results_rel_den = pd.concat([model_results_rel_den, results_rel_den], axis = 1)\n",
    "\n",
    "    # Assuming model_results_rel_den is already defined\n",
    "    average_mse_df = pd.DataFrame(model_results_rel_den['MSE'].T.mean(), columns=['Average_MSE']).reset_index(drop=True)\n",
    "    average_std_df = pd.DataFrame(model_results_rel_den['MSE'].T.std(), columns=['std']).reset_index(drop=True)\n",
    "    \n",
    "    # Concatenate the original DataFrame with the new one\n",
    "    result = pd.concat([\n",
    "                        model_results_rel_den.iloc[:, 0:1], \n",
    "                        average_mse_df,\n",
    "                        average_std_df\n",
    "                        ], \n",
    "                        axis=1)\n",
    "    \n",
    "    # Get current date and time\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime(\"%B_%d_%Y_%H_%M\")\n",
    "\n",
    "    # Save Data\n",
    "    result.to_csv(f'./results/model_average_performance_{filename}_{date_str}.csv')\n",
    "\n",
    "    # Print the Table\n",
    "    # result.sort_values(by='Average_MSE')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rel_den = perform_cross_validate(X, \n",
    "                                        y_rel_den, \n",
    "                                        numerical_features, \n",
    "                                        categorical_features, \n",
    "                                        reordered_features, \n",
    "                                        filename = 'relative_density', \n",
    "                                        random_state = 400, # None when 'shuffle' is 'False'\n",
    "                                        shuffle = True, # 200\n",
    "                                        folds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rel_den.sort_values(by='Average_MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sam_shrink = perform_cross_validate(X, \n",
    "                                           y_sam_shrink, \n",
    "                                           numerical_features, \n",
    "                                           categorical_features, \n",
    "                                           reordered_features, \n",
    "                                           filename = 'sample_shrinkage', \n",
    "                                           random_state = 420, \n",
    "                                           shuffle = True,\n",
    "                                           folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sam_shrink.sort_values(by='Average_MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Explanation for Relative Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed_rel_den, X_test_transformed_rel_den, y_train_rel_den, y_test_rel_den, preprocessor_pipeline = pre_process_data( numerical_features, \n",
    "                                                                                                                                    categorical_features, \n",
    "                                                                                                                                    X, y_rel_den, \n",
    "                                                                                                                                    columns_to_use, \n",
    "                                                                                                                                    random_state = 420, \n",
    "                                                                                                                                    test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import shap\n",
    "\n",
    "# Create and train BaggingRegressor\n",
    "base_estimator = DecisionTreeRegressor(max_depth=3)\n",
    "model = BaggingRegressor(\n",
    "    estimator=base_estimator,\n",
    "    n_estimators = 100,\n",
    "    max_samples = 0.5,\n",
    "    max_features = 1\n",
    ")\n",
    "model.fit(X_train_transformed_rel_den, y_train_rel_den)\n",
    "\n",
    "# Calculate SHAP values for a subset of data\n",
    "background_data = X_train_transformed_rel_den\n",
    "\n",
    "# Create explainer using KernelExplainer\n",
    "explainer = shap.KernelExplainer(model.predict, background_data)\n",
    "\n",
    "# Get shap values for display\n",
    "display_data = X_train_transformed_rel_den  \n",
    "shap_display = explainer(display_data)  # This creates an Explanation object\n",
    "\n",
    "# Visualizations\n",
    "# Summary plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_display.values, display_data)\n",
    "\n",
    "# Feature importance bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_display.values, display_data, plot_type=\"bar\")\n",
    "\n",
    "# Heatmap of SHAP values\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.plots.heatmap(shap_display)\n",
    "\n",
    "# Dependence plot for most important feature\n",
    "# feature_importance = np.abs(shap_display.values).mean(0)\n",
    "# most_important_feature = np.argmax(feature_importance)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# shap.dependence_plot(most_important_feature, shap_display.values, display_data)\n",
    "\n",
    "# Individual prediction explanation\n",
    "plt.figure(figsize=(8, 3))\n",
    "shap.force_plot(\n",
    "                explainer.expected_value,\n",
    "                shap_display.values[0],\n",
    "                display_data.iloc[0],\n",
    "                matplotlib=True,\n",
    "                show=False\n",
    "                )\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Explanation for sample shrinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed_sam_shrink, X_test_transformed_sam_shrink, y_train_sam_shrink, y_test_sam_shrink, preprocessor_pipeline = pre_process_data( numerical_features, \n",
    "                                                                                                                                                categorical_features, \n",
    "                                                                                                                                                X, y_sam_shrink, \n",
    "                                                                                                                                                columns_to_use, \n",
    "                                                                                                                                                random_state = 420, \n",
    "                                                                                                                                                test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import shap\n",
    "\n",
    "# Create and train LinearRegression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_transformed_sam_shrink, y_train_sam_shrink)\n",
    "\n",
    "# Create explainer - for linear models, we can use LinearExplainer\n",
    "explainer = shap.LinearExplainer(model, X_train_transformed_sam_shrink)\n",
    "\n",
    "# Get shap values for display\n",
    "display_data = X_train_transformed_sam_shrink[:50]  # Using 50 samples for visualization\n",
    "shap_display = explainer(display_data)\n",
    "\n",
    "# Visualizations\n",
    "# Summary plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_display.values, display_data)\n",
    "\n",
    "# Feature importance bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_display.values, display_data, plot_type=\"bar\")\n",
    "\n",
    "# Heatmap of SHAP values\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.plots.heatmap(shap_display)\n",
    "\n",
    "# # Dependence plot for most important feature\n",
    "# feature_importance = np.abs(shap_display.values).mean(0)\n",
    "# most_important_feature = np.argmax(feature_importance)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# shap.dependence_plot(most_important_feature, shap_display.values, display_data)\n",
    "\n",
    "# Individual prediction explanation\n",
    "plt.figure(figsize=(8, 3))\n",
    "shap.force_plot(\n",
    "    explainer.expected_value,\n",
    "    shap_display.values[0],\n",
    "    display_data.iloc[0],\n",
    "    matplotlib=True,\n",
    "    show=False\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature importance summary\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train_transformed_sam_shrink.columns,\n",
    "    'Importance': np.abs(shap_display.values).mean(0)\n",
    "})\n",
    "print(\"\\nFeature Importance Summary:\")\n",
    "print(feature_importance_df.sort_values('Importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
